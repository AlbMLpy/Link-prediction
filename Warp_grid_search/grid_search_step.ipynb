{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8536391-6a9a-4e8f-b706-d3eaa3a615cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # best linear algebra\n",
    "import torch # same as numpy + GPU\n",
    "from torch import optim # different optimizers\n",
    "from torch.optim.lr_scheduler import StepLR # adjust the learning rate\n",
    "from sklearn.preprocessing import normalize # to normalize final factor matrices\n",
    "\n",
    "import argparse # add terminal arguments\n",
    "import sys\n",
    "sys.path.insert(1, '/notebook/Link-prediction/gpu/') # insert at 1, 0 is the script path (or '' in REPL)\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from ipypb import track\n",
    "\n",
    "from t_alg import mttcrp, mttcrp1, get_elem_deriv_tensor\n",
    "from t_alg import factors_to_tensor, gcp_grad, multi_ind_to_indices, indices_to_multi_ind\n",
    "from samplings import give_ns, generate_data\n",
    "from elementwise_grads import bernoulli_logit_loss, bernoulli_logit_loss_grad, bernoulli_loss, bernoulli_loss_grad\n",
    "from general_functions1 import sqrt_err_relative, check_coo_tensor, gen_coo_tensor\n",
    "from general_functions1 import create_filter, hr\n",
    "from decimal import Decimal\n",
    "from experiments import data_storage, Trainer, run_epoch\n",
    "from model import FoxIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604167e1-e31b-4619-90ab-e57c777ce7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util import import_source_as_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d44994-d383-40c0-87dc-de1ac599d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "\n",
    "@static_vars(fail_count=0)\n",
    "def check_early_stop(target_score, previous_best, margin=0, max_attempts=1000):\n",
    "    if (previous_best > target_score):\n",
    "        previous_best = target_score\n",
    "    if (margin >= 0) and (target_score > previous_best + margin):\n",
    "        check_early_stop.fail_count += 1\n",
    "    else:\n",
    "        check_early_stop.fail_count = 0\n",
    "    if check_early_stop.fail_count >= max_attempts:\n",
    "        print('Interrupted due to early stopping condition.', check_early_stop.fail_count, flush = True)\n",
    "        raise StopIteration\n",
    "\n",
    "@static_vars(fail_count_score=0)        \n",
    "def check_early_stop_score(target_score, previous_best, margin=0, max_attempts=3000):\n",
    "    if (previous_best > target_score):\n",
    "        previous_best = target_score\n",
    "    if (margin >= 0) and (target_score < previous_best + margin):\n",
    "        check_early_stop_score.fail_count_score += 1\n",
    "    else:\n",
    "        check_early_stop_score.fail_count_score = 0\n",
    "    if check_early_stop_score.fail_count_score >= max_attempts:\n",
    "        print('Interrupted due to early stopping scoring condition.', check_early_stop_score.fail_count_score, flush = True)\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b32a43-c2c4-4c96-9c20-7c0f862593d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in yaml\n",
    "# num_epoch = args.n_epoch\n",
    "# rank = args.dim \n",
    "# lr = args.lr\n",
    "# batch_size = args.batch_size\n",
    "# step_size=args.scheduler_step\n",
    "# gamma=args.scheduler_gamma\n",
    "# momentum = args.momentum\n",
    "# opt_type = args.opt_type (SGD, ADAM, AdamW)\n",
    "# output_file = args.out_file\n",
    "\n",
    "import_source_as_module('/notebook/Relations_Learning/grid_search/configs/adam_grid.py')\n",
    "cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "import adam_grid  # python file with default hyperparameters\n",
    "# Set up your default hyperparameters\n",
    "hyperparameters = adam_grid\n",
    "\n",
    "# Pass them wandb.init\n",
    "wandb.init(project = 'FOxIE', entity = 'sayankotor')\n",
    "# Access all hyperparameter values through wandb.config\n",
    "#config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c6505e-2a2c-444d-81f3-9b141f1fdc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--out_file'], dest='out_file', nargs=None, const=None, default='/notebook/Link-prediction/Warp_grid_search/output/result.txx', type=<class 'str'>, choices=None, help='path tot output_file', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epoch\", type=int, required=True, default=200)\n",
    "parser.add_argument(\"--lr\", type=float, required=True) # depends on choice of data pack\n",
    "parser.add_argument(\"--path_data\", type=str, default=\"/notebook/Relations_Learning/Link_Prediction_Data/FB15K237/\")\n",
    "#parser.add_argument(\"--path_filters\", type=str, default=\"/notebook/Relations_Learning/\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--opt_type\", type=str, default='adam')\n",
    "\n",
    "parser.add_argument('--dim', type = int, default = 200)\n",
    "parser.add_argument('--l2', type = float, default = 0.0)\n",
    "parser.add_argument('--scheduler_step', type=int, default=2, help=\"Scheduler step size\")\n",
    "parser.add_argument(\"--scheduler_gamma\", type=float, default = 0.5, help=\"scheduler_gamma\")\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum in Sgd')\n",
    "parser.add_argument('--nesterov', type=bool, default=False, help='nesterov momentum in Sgd')\n",
    "parser.add_argument(\n",
    "    '--out_file', type=str,\n",
    "    default='/notebook/Link-prediction/Warp_grid_search/output/result.txx',\n",
    "    help='path tot output_file',\n",
    ")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9888f6-a470-400a-b1e9-75afbfd2e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acd967f6-cbab-415a-b41c-1170be9674c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/output/\" + \"result_\" + str(datetime.datetime.now())+\".txt\" # files for text output\n",
    "#output_folder = args.out_folder # folder where best factors are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51ed0688-fd78-4a99-9af5-598bd371b884",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/output/result_2021-07-26 16:23:17.280361.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2772/3732984646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/output/result_2021-07-26 16:23:17.280361.txt'"
     ]
    }
   ],
   "source": [
    "file_out = open(output_file, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4e7c5-0c23-424b-9570-d9ac32ba4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = args.path_data\n",
    "path_filters = args.path_filters\n",
    "#if ('path_data' in args.keys()):\n",
    "    #path_data = args.path_data\n",
    "    #file_out.write(\"Default data path %s exist %r\", path_data, os.path.isdir(path_data), flush = True)\n",
    "#else:\n",
    "    #file_out.write(\"Default data path \", path_data, flush = True)\n",
    "    #path_data = \"/notebook/Relations_Learning/Link_Prediction_Data/FB15K237/\"\n",
    "    \n",
    "with open(path_filters + 'test_filter.pkl', 'rb') as f:\n",
    "    test_filter = pickle.load(f)\n",
    "    \n",
    "with open(path_filters + 'valid_filter.pkl', 'rb') as f:\n",
    "    valid_filter = pickle.load(f)\n",
    "        \n",
    "entity_list = pickle.load(open(path_data + 'entity_list', 'rb'))\n",
    "relation_list = pickle.load(open(path_data + 'relation_list', 'rb'))\n",
    "\n",
    "train_triples = pickle.load(open(path_data + 'train_triples', 'rb'))\n",
    "valid_triples = pickle.load(open(path_data + 'valid_triples', 'rb'))\n",
    "test_triples = pickle.load(open(path_data + 'test_triples', 'rb'))\n",
    "train_valid_triples = pickle.load(open(path_data + 'train_valid_triples', 'rb'))\n",
    "\n",
    "entity_map = pickle.load(open(path_data + 'entity_map', 'rb'))\n",
    "relation_map = pickle.load(open(path_data + 'relation_map', 'rb'))\n",
    "\n",
    "all_triples = train_valid_triples + test_triples\n",
    "\n",
    "file_out.write(\"loaded1_\\n\")\n",
    "num_epoch = args.n_epoch\n",
    "rank = args.dim \n",
    "lr = args.lr\n",
    "batch_size = args.batch_size\n",
    "    \n",
    "    \n",
    "seed = 13 \n",
    "how_many = 2\n",
    "l2 = args.l2\n",
    "    \n",
    "values = [1] * len(train_triples)\n",
    "values = np.array(values, dtype=np.int64)\n",
    "\n",
    "coords = np.array(train_triples, dtype=np.int64)\n",
    "nnz = len(train_triples)\n",
    "data_shape = (len(entity_list), len(relation_list), len(entity_list))\n",
    "    \n",
    "print (data_shape, flush = True)\n",
    "    \n",
    "coo_tensor = coords\n",
    "vals = values\n",
    "shape = data_shape\n",
    "\n",
    "num_epoch = args.n_epoch\n",
    "\n",
    "random_state = np.random.seed(seed)\n",
    "\n",
    "# specify property of data\n",
    "init_mind_set = set(indices_to_multi_ind(coo_tensor, shape))\n",
    "coo_ns = np.empty((how_many * len(init_mind_set) + vals.size, 3), dtype=np.int64)\n",
    "vals_ns = np.empty((how_many * len(init_mind_set) + vals.size,), dtype=np.float64)\n",
    "    \n",
    "data_s = data_storage(sparse_coords = coords, sparse_vals =values, mind_set = init_mind_set, shape=data_shape, how_many=2, valid_filters = valid_filter, valid_triples = valid_triples)\n",
    "\n",
    "# specify property of training\n",
    "err_arr = []\n",
    "error = 0.0\n",
    "it = 0\n",
    "previous_best_loss = 100000.0\n",
    "best_tuple = (0.0, 0.0, 0.0, 0.0)\n",
    "best_hit_10 = 0.0\n",
    "# specify training class\n",
    "trainer = Trainer(best_hit_10, previous_best_loss, err_arr, it)\n",
    "    \n",
    "start = timer()\n",
    "\n",
    "model = FoxIE(rank=rank, shape=data_shape, given_loss=bernoulli_logit_loss, given_loss_grad=bernoulli_logit_loss_grad, device=device)\n",
    "model.init()\n",
    "\n",
    "\n",
    "score_margin_ = 0.01\n",
    "score_attempts_ = 15\n",
    "optimizer = optim.Adam([model.a_torch, model.b_torch], lr = args.lr)\n",
    "\n",
    "if (args.opt_type == 'sdg'):\n",
    "    score_margin_ = 0.0001\n",
    "    score_attempts_ = 25\n",
    "    optimizer = optim.SGD([model.a_torch, model.b_torch], lr = args.lr, momentum = args.momentum)\n",
    "          \n",
    "elif (args.opt_type == 'adamw'):\n",
    "    score_margin_ = 0.01\n",
    "    score_attempts_ = 15\n",
    "    optimizer = optim.AdamW([model.a_torch, model.b_torch], lr = args.lr)\n",
    "          \n",
    "scheduler = StepLR(optimizer, step_size=args.scheduler_step, gamma=args.scheduler_gamma)\n",
    "\n",
    "show_iter = True\n",
    "start = timer()\n",
    "for epoch in range(num_epoch):\n",
    "    try:\n",
    "        d = 6\n",
    "        run_epoch(data_s, epoch, device, model, optimizer, scheduler, batch_size, trainer, show_iter = True, fout = file_out)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        break\n",
    "        print (\"early_stoping loss\", flush = True)\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "    hit3, hit5, hit10, mrr = model.evaluate(data_s)\n",
    "\n",
    "    metrics = {\"hit3\": hit3,\n",
    "                \"hit5\": hit5,\n",
    "                \"hit10\": hit10,\n",
    "                \"mrr\":mrr,\n",
    "                \"loss\":np.mean(trainer.err_arr[len(trainer.err_arr) - 5:])\n",
    "              }\n",
    "    wandb.log(metrics)\n",
    "    print (hit3, hit5, hit10, mrr, flush = True)\n",
    "\n",
    "    file_out.write('%s %s %s %s \\n' % (hit3, hit5, hit10, mrr))\n",
    "    file_out.flush()\n",
    "        # early stopping by hit@10\n",
    "    try:\n",
    "        check_early_stop_score(hit10, best_hit_10, margin=score_margin_, max_attempts=score_attempts_)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        end = timer()\n",
    "        time = end - start\n",
    "        file_out.write(\"\\n\")\n",
    "        file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "        file_out.write(\"early_stoping score\")\n",
    "        file_out.write(\"Best scores \", best_tuple)\n",
    "        file_out.flush()\n",
    "        file_out.close()\n",
    "        break\n",
    "        print (\"early_stoping score\", flush = True)\n",
    "\n",
    "    # if hit@10 grows update checkpoint\n",
    "    if (hit10 > best_hit_10):\n",
    "        best_hit_10 = hit10\n",
    "        best_tuple = (hit3, hit5, hit10, mrr)\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_a.npz', a_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_b.npz', b_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_c.npz', a_torch.cpu().data.numpy())\n",
    "    \n",
    "end = timer()\n",
    "time = end - start\n",
    "file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "file_out.write(\"Best scores %s %s %s %s \\n\" % (best_tuple[0],best_tuple[1],best_tuple[2],best_tuple[3]))\n",
    "file_out.write(\"\\n\")\n",
    "file_out.flush()\n",
    "file_out.close()\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
