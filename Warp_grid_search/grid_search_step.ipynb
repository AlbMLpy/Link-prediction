{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8536391-6a9a-4e8f-b706-d3eaa3a615cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # best linear algebra\n",
    "import torch # same as numpy + GPU\n",
    "from torch import optim # different optimizers\n",
    "from torch.optim.lr_scheduler import StepLR # adjust the learning rate\n",
    "from sklearn.preprocessing import normalize # to normalize final factor matrices\n",
    "\n",
    "import argparse # add terminal arguments\n",
    "import sys; sys.path.append('/notebook/Link-prediction/gpu/') # to use files from this directory\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from ipypb import track\n",
    "\n",
    "import evaluation_functions as ef\n",
    "\n",
    "\n",
    "\n",
    "from t_alg import mttcrp, mttcrp1, get_elem_deriv_tensor\n",
    "from t_alg import factors_to_tensor, gcp_grad, multi_ind_to_indices, indices_to_multi_ind\n",
    "from samplings import give_ns, generate_data\n",
    "from elementwise_grads import bernoulli_logit_loss, bernoulli_logit_loss_grad, bernoulli_loss, bernoulli_loss_grad\n",
    "from general_functions1 import sqrt_err_relative, check_coo_tensor, gen_coo_tensor\n",
    "from general_functions1 import create_filter, hr\n",
    "from decimal import Decimal\n",
    "from experiments import data_storage, Trainer, run_epoch\n",
    "from model import FoxIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ed4cc-2c92-4731-8042-3de42657b15a",
   "metadata": {},
   "source": [
    "import wandb\n",
    "from util import import_source_as_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d44994-d383-40c0-87dc-de1ac599d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "\n",
    "@static_vars(fail_count=0)\n",
    "def check_early_stop(target_score, previous_best, margin=0, max_attempts=1000):\n",
    "    if (previous_best > target_score):\n",
    "        previous_best = target_score\n",
    "    if (margin >= 0) and (target_score > previous_best + margin):\n",
    "        check_early_stop.fail_count += 1\n",
    "    else:\n",
    "        check_early_stop.fail_count = 0\n",
    "    if check_early_stop.fail_count >= max_attempts:\n",
    "        print('Interrupted due to early stopping condition.', check_early_stop.fail_count, flush = True)\n",
    "        raise StopIteration\n",
    "\n",
    "@static_vars(fail_count_score=0)        \n",
    "def check_early_stop_score(target_score, previous_best, margin=0, max_attempts=3000):\n",
    "    if (previous_best > target_score):\n",
    "        previous_best = target_score\n",
    "    if (margin >= 0) and (target_score < previous_best + margin):\n",
    "        check_early_stop_score.fail_count_score += 1\n",
    "    else:\n",
    "        check_early_stop_score.fail_count_score = 0\n",
    "    if check_early_stop_score.fail_count_score >= max_attempts:\n",
    "        print(\n",
    "            'Interrupted due to early stopping scoring condition.',\n",
    "            check_early_stop_score.fail_count_score, flush = True,\n",
    "        )\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906f981-56c2-4a4b-9c67-5882f0579f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in yaml\n",
    "# num_epoch = args.n_epoch\n",
    "# rank = args.dim \n",
    "# lr = args.lr\n",
    "# batch_size = args.batch_size\n",
    "# step_size=args.scheduler_step\n",
    "# gamma=args.scheduler_gamma\n",
    "# momentum = args.momentum\n",
    "# opt_type = args.opt_type (SGD, ADAM, AdamW)\n",
    "# output_file = args.out_file\n",
    "\n",
    "#import_source_as_module('/notebook/Relations_Learning/grid_search/configs/adam_grid.py')\n",
    "cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "#import adam_grid  # python file with default hyperparameters\n",
    "# Set up your default hyperparameters\n",
    "#hyperparameters = adam_grid\n",
    "\n",
    "# Pass them wandb.init\n",
    "#wandb.init(project = 'FOxIE', entity = 'sayankotor')\n",
    "# Access all hyperparameter values through wandb.config\n",
    "#config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10eb55-847d-42d9-bbfd-70aa06bd91c0",
   "metadata": {},
   "source": [
    "## Parse the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c6505e-2a2c-444d-81f3-9b141f1fdc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--out_file'], dest='out_file', nargs=None, const=None, default='/notebook/Link-prediction/Warp_grid_search/output/result.txx', type=<class 'str'>, choices=None, help='path tot output_file', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epoch\", type=int, required=True, default=200)\n",
    "parser.add_argument(\"--lr\", type=float, required=True) # depends on choice of data pack\n",
    "parser.add_argument(\"--path_data\", type=str, default=\"/notebook/Relations_Learning/Link_Prediction_Data/FB15K237/\")\n",
    "#parser.add_argument(\"--path_filters\", type=str, default=\"/notebook/Relations_Learning/\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--opt_type\", type=str, default='adam')\n",
    "\n",
    "parser.add_argument('--dim', type = int, default = 200)\n",
    "parser.add_argument('--l2', type = float, default = 0.0)\n",
    "parser.add_argument('--scheduler_step', type=int, default=2, help=\"Scheduler step size\")\n",
    "parser.add_argument(\"--scheduler_gamma\", type=float, default = 0.5, help=\"scheduler_gamma\")\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum in Sgd')\n",
    "parser.add_argument('--nesterov', type=bool, default=False, help='nesterov momentum in Sgd')\n",
    "parser.add_argument(\n",
    "    '--out_file', type=str,\n",
    "    default='/notebook/Link-prediction/Warp_grid_search/output/result.txt',\n",
    "    help='path tot output_file',\n",
    ")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2354b636-4406-4831-b053-84e92dd385ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, n_epoch, lr,\n",
    "                 path_data=\"/notebook/Relations_Learning/Link_Prediction_Data/FB15K237/\",\n",
    "                 batch_size=32, opt_type='adam', dim=200, l2=0.0, scheduler_step=2,\n",
    "                 scheduler_gamma=0.5, momentum=0.9, weight_decay=0.1, nesterov=False, how_many=2,\n",
    "                 out_file='/notebook/Link-prediction/Warp_grid_search/output/result.txt'):\n",
    "        self.n_epoch = n_epoch\n",
    "        self.lr = lr\n",
    "        self.path_data = path_data\n",
    "        self.batch_size = batch_size\n",
    "        self.opt_type = opt_type\n",
    "        self.dim = dim\n",
    "        self.l2 = l2\n",
    "        self.scheduler_step = scheduler_step\n",
    "        self.scheduler_gamma = scheduler_gamma\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov = nesterov\n",
    "        self.how_many = how_many\n",
    "        self.out_file = out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1979d62b-92ee-408c-9248-cbe0111ac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Parser(\n",
    "    n_epoch=50,\n",
    "    lr=0.0001,\n",
    "    path_data=\"/notebook/Relations_Learning/Link_Prediction_Data/FB15K237/\",\n",
    "    batch_size=32,\n",
    "    opt_type='adam',\n",
    "    dim=200,\n",
    "    l2=0.0,\n",
    "    scheduler_step=2,\n",
    "    scheduler_gamma=0.5,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.01,\n",
    "    nesterov=False,\n",
    "    how_many=2,\n",
    "    out_file='/notebook/Link-prediction/Warp_grid_search/output/result.txt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253457e-bd2c-483d-a6e2-dcb0a9f936d0",
   "metadata": {},
   "source": [
    "## Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b747fa52-0b69-4e80-8e5c-13ec37460f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebook/Link-prediction/Warp_grid_search'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acd967f6-cbab-415a-b41c-1170be9674c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = '/home/asayapin/Link-prediction/Warp_grid_search'\n",
    "\n",
    "output_file = os.getcwd() + \"/output\" + \"/result_\" + str(datetime.datetime.now()) + \".txt\" # files for text output\n",
    "#output_folder = args.out_folder # folder where best factors are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51ed0688-fd78-4a99-9af5-598bd371b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = open(output_file, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "090eef0f-7fba-4d7c-a252-9ee0e5fa1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = args.path_data \n",
    "\n",
    "entity_list = pickle.load(open(path_data + 'entity_list', 'rb'))\n",
    "relation_list = pickle.load(open(path_data + 'relation_list', 'rb'))\n",
    "\n",
    "train_triples = pickle.load(open(path_data + 'train_triples', 'rb'))\n",
    "valid_triples = pickle.load(open(path_data + 'valid_triples', 'rb'))\n",
    "test_triples = pickle.load(open(path_data + 'test_triples', 'rb'))\n",
    "train_valid_triples = pickle.load(open(path_data + 'train_valid_triples', 'rb'))\n",
    "\n",
    "entity_map = pickle.load(open(path_data + 'entity_map', 'rb'))\n",
    "relation_map = pickle.load(open(path_data + 'relation_map', 'rb'))\n",
    "\n",
    "all_triples = train_valid_triples + test_triples\n",
    "\n",
    "sample_positive = ef.create_filter(train_triples)\n",
    "ft = ef.create_filter(all_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e7665f3-9c57-43db-8efd-bac491cb1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out.write(\"loaded1_\\n\")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epoch = args.n_epoch\n",
    "rank = args.dim \n",
    "lr = args.lr\n",
    "batch_size = args.batch_size \n",
    "how_many = args.how_many\n",
    "l2 = args.l2\n",
    "\n",
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6dcd990e-7bb8-4cca-95b3-8d30a31639ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14541, 237, 14541)\n"
     ]
    }
   ],
   "source": [
    "vals = [1] * len(train_triples)\n",
    "vals = np.array(vals, dtype=np.int64)\n",
    "\n",
    "coo_tensor = np.array(train_triples, dtype=np.int64)\n",
    "nnz = len(train_triples)\n",
    "shape = (len(entity_list), len(relation_list), len(entity_list))\n",
    "    \n",
    "print(shape, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9212ad1-d5ef-4b65-9290-b494f888280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GCP_WARP\n",
    "import WARPLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4cc99341-05c5-449e-92ca-8bb4bc824cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCP_WARP()\n"
     ]
    }
   ],
   "source": [
    "n_entity = shape[0]\n",
    "n_relation = shape[1]\n",
    "n_factors = rank\n",
    "\n",
    "model = GCP_WARP.GCP_WARP(n_entity, n_relation, n_factors, device=device).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926fd4f-1539-4e57-8932-739dd1fa0909",
   "metadata": {},
   "source": [
    "def train_batch_warp(train_data, data_shape, sample_positive, model,\n",
    "                     loss_fn, optimizer, un_pair, bs=64,\n",
    "                     show=True, device='cpu'):\n",
    "\n",
    "    user_idx = np.arange(len(un_pair))\n",
    "    np.random.shuffle(user_idx)\n",
    "    batches = np.array_split(user_idx, train_data.shape[0] // bs)\n",
    "    cols = torch.arange(data_shape[2])\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        rows = torch.tensor(batch)\n",
    "\n",
    "        # Compute prediction error\n",
    "        rating = torch.zeros((len(un_pair[rows]), data_shape[2])).to(device)\n",
    "        for j in range(rating.shape[0]):\n",
    "            rating[j][sample_positive[tuple(un_pair[rows][j].tolist())]] = 1.0\n",
    "\n",
    "        prediction = model(\n",
    "            torch.repeat_interleave(torch.tensor(un_pair[rows][:, 0]), cols.shape[0]),\n",
    "            torch.repeat_interleave(torch.tensor(un_pair[rows][:, 1]), cols.shape[0]),\n",
    "            torch.tile(cols, (un_pair[rows].shape[0], )),\n",
    "        ).view(un_pair[rows].shape[0], cols.shape[0]).to(torch.float64)\n",
    "\n",
    "        loss = loss_fn(prediction, rating)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if show and (i % 10 == 0):\n",
    "            loss, current = loss.item(), i * len(rows)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{coords.shape[0]:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e21bd-09fb-4fe9-85ec-2b3174379c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.opt_type == 'adam'):\n",
    "    score_margin_ = 0.01\n",
    "    score_attempts_ = 15\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr) #lr=5e-4\n",
    "\n",
    "elif (args.opt_type == 'sdg'):\n",
    "    score_margin_ = 0.0001\n",
    "    score_attempts_ = 25\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=args.lr, momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "          \n",
    "elif (args.opt_type == 'adamw'):\n",
    "    score_margin_ = 0.01\n",
    "    score_attempts_ = 15\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=args.scheduler_step, gamma=args.scheduler_gamma)\n",
    "    \n",
    "loss_fn = WARPLoss(20)\n",
    "un_pair = np.array(list(sample_positive.keys()))\n",
    "hr_k = (1, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee0faf-aa93-4513-a381-65f479e6c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_iter = True\n",
    "start = timer()\n",
    "for epoch in range(num_epoch):\n",
    "    try:\n",
    "        run_epoch(data_s, epoch, device, model, optimizer, scheduler, batch_size, trainer, show_iter = True, fout = file_out)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        break\n",
    "        print (\"early_stoping loss\", flush = True)\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "    hit3, hit5, hit10, mrr = model.evaluate(data_s)\n",
    "\n",
    "    metrics = {\"hit3\": hit3,\n",
    "                \"hit5\": hit5,\n",
    "                \"hit10\": hit10,\n",
    "                \"mrr\":mrr,\n",
    "                \"loss\":np.mean(trainer.err_arr[len(trainer.err_arr) - 5:])\n",
    "              }\n",
    "    wandb.log(metrics)\n",
    "    print (hit3, hit5, hit10, mrr, flush = True)\n",
    "\n",
    "    file_out.write('%s %s %s %s \\n' % (hit3, hit5, hit10, mrr))\n",
    "    file_out.flush()\n",
    "        # early stopping by hit@10\n",
    "    try:\n",
    "        check_early_stop_score(hit10, best_hit_10, margin=score_margin_, max_attempts=score_attempts_)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        end = timer()\n",
    "        time = end - start\n",
    "        file_out.write(\"\\n\")\n",
    "        file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "        file_out.write(\"early_stoping score\")\n",
    "        file_out.write(\"Best scores \", best_tuple)\n",
    "        file_out.flush()\n",
    "        file_out.close()\n",
    "        break\n",
    "        print (\"early_stoping score\", flush = True)\n",
    "\n",
    "    # if hit@10 grows update checkpoint\n",
    "    if (hit10 > best_hit_10):\n",
    "        best_hit_10 = hit10\n",
    "        best_tuple = (hit3, hit5, hit10, mrr)\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_a.npz', a_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_b.npz', b_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_c.npz', a_torch.cpu().data.numpy())\n",
    "    \n",
    "end = timer()\n",
    "time = end - start\n",
    "file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "file_out.write(\"Best scores %s %s %s %s \\n\" % (best_tuple[0],best_tuple[1],best_tuple[2],best_tuple[3]))\n",
    "file_out.write(\"\\n\")\n",
    "file_out.flush()\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4e7c5-0c23-424b-9570-d9ac32ba4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.seed(seed)\n",
    "\n",
    "# specify property of data\n",
    "init_mind_set = set(indices_to_multi_ind(coo_tensor, shape))\n",
    "coo_ns = np.empty((how_many * len(init_mind_set) + vals.size, 3), dtype=np.int64)\n",
    "vals_ns = np.empty((how_many * len(init_mind_set) + vals.size,), dtype=np.float64)\n",
    "    \n",
    "data_s = data_storage(sparse_coords = coords, sparse_vals =values, mind_set = init_mind_set, shape=data_shape, how_many=2, valid_filters = valid_filter, valid_triples = valid_triples)\n",
    "\n",
    "# specify property of training\n",
    "err_arr = []\n",
    "error = 0.0\n",
    "it = 0\n",
    "previous_best_loss = 100000.0\n",
    "best_tuple = (0.0, 0.0, 0.0, 0.0)\n",
    "best_hit_10 = 0.0\n",
    "# specify training class\n",
    "trainer = Trainer(best_hit_10, previous_best_loss, err_arr, it)\n",
    "    \n",
    "start = timer()\n",
    "\n",
    "model = FoxIE(rank=rank, shape=data_shape, given_loss=bernoulli_logit_loss, given_loss_grad=bernoulli_logit_loss_grad, device=device)\n",
    "model.init()\n",
    "\n",
    "\n",
    "score_margin_ = 0.01\n",
    "score_attempts_ = 15\n",
    "optimizer = optim.Adam([model.a_torch, model.b_torch], lr = args.lr)\n",
    "\n",
    "if (args.opt_type == 'sdg'):\n",
    "    score_margin_ = 0.0001\n",
    "    score_attempts_ = 25\n",
    "    optimizer = optim.SGD([model.a_torch, model.b_torch], lr = args.lr, momentum = args.momentum)\n",
    "          \n",
    "elif (args.opt_type == 'adamw'):\n",
    "    score_margin_ = 0.01\n",
    "    score_attempts_ = 15\n",
    "    optimizer = optim.AdamW([model.a_torch, model.b_torch], lr = args.lr)\n",
    "          \n",
    "scheduler = StepLR(optimizer, step_size=args.scheduler_step, gamma=args.scheduler_gamma)\n",
    "\n",
    "show_iter = True\n",
    "start = timer()\n",
    "for epoch in range(num_epoch):\n",
    "    try:\n",
    "        d = 6\n",
    "        run_epoch(data_s, epoch, device, model, optimizer, scheduler, batch_size, trainer, show_iter = True, fout = file_out)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        break\n",
    "        print (\"early_stoping loss\", flush = True)\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "    hit3, hit5, hit10, mrr = model.evaluate(data_s)\n",
    "\n",
    "    metrics = {\"hit3\": hit3,\n",
    "                \"hit5\": hit5,\n",
    "                \"hit10\": hit10,\n",
    "                \"mrr\":mrr,\n",
    "                \"loss\":np.mean(trainer.err_arr[len(trainer.err_arr) - 5:])\n",
    "              }\n",
    "    wandb.log(metrics)\n",
    "    print (hit3, hit5, hit10, mrr, flush = True)\n",
    "\n",
    "    file_out.write('%s %s %s %s \\n' % (hit3, hit5, hit10, mrr))\n",
    "    file_out.flush()\n",
    "        # early stopping by hit@10\n",
    "    try:\n",
    "        check_early_stop_score(hit10, best_hit_10, margin=score_margin_, max_attempts=score_attempts_)\n",
    "    except StopIteration: # early stopping condition met\n",
    "        end = timer()\n",
    "        time = end - start\n",
    "        file_out.write(\"\\n\")\n",
    "        file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "        file_out.write(\"early_stoping score\")\n",
    "        file_out.write(\"Best scores \", best_tuple)\n",
    "        file_out.flush()\n",
    "        file_out.close()\n",
    "        break\n",
    "        print (\"early_stoping score\", flush = True)\n",
    "\n",
    "    # if hit@10 grows update checkpoint\n",
    "    if (hit10 > best_hit_10):\n",
    "        best_hit_10 = hit10\n",
    "        best_tuple = (hit3, hit5, hit10, mrr)\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_a.npz', a_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_b.npz', b_torch.cpu().data.numpy())\n",
    "        #np.save('/notebook/Relations_Learning/gpu/gpu_c.npz', a_torch.cpu().data.numpy())\n",
    "    \n",
    "end = timer()\n",
    "time = end - start\n",
    "file_out.write(\"In %s epoch; time %s \\n\" % (epoch, time))\n",
    "file_out.write(\"Best scores %s %s %s %s \\n\" % (best_tuple[0],best_tuple[1],best_tuple[2],best_tuple[3]))\n",
    "file_out.write(\"\\n\")\n",
    "file_out.flush()\n",
    "file_out.close()\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
